{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n!pip install --no-deps cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer comet_ml==3.48.1\n!pip install --no-deps unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T09:16:45.302832Z","iopub.execute_input":"2025-07-27T09:16:45.302983Z","iopub.status.idle":"2025-07-27T09:17:01.605083Z","shell.execute_reply.started":"2025-07-27T09:16:45.302968Z","shell.execute_reply":"2025-07-27T09:17:01.604357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom getpass import getpass\nhf_token = \"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\"\nenable_hf = bool(hf_token)\nprint(f\"Is Hugging Face enabled? '{enable_hf}'\")\n\ncomet_api_key = \"LHVLuczJDQUM8l4jzZVtRrpue\"\nenable_comet = bool(comet_api_key)\ncomet_project_name = \"second-brain-course\"\nprint(f\"Is Comet enabled? '{enable_comet}'\")\n\nif enable_hf:\n    os.environ[\"HF_TOKEN\"] = hf_token\nif enable_comet:\n    os.environ[\"COMET_API_KEY\"] = comet_api_key\n    os.environ[\"COMET_PROJECT_NAME\"] = comet_project_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:25:33.885162Z","iopub.execute_input":"2025-07-27T03:25:33.885408Z","iopub.status.idle":"2025-07-27T03:25:33.891317Z","shell.execute_reply.started":"2025-07-27T03:25:33.885386Z","shell.execute_reply":"2025-07-27T03:25:33.890578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\ndef get_gpu_info() -> str | None:\n    \"\"\"Gets GPU device name if available.\n\n    Returns:\n        str | None: Name of the GPU device if available, None if no GPU is found.\n    \"\"\"\n    if not torch.cuda.is_available():\n        return None\n\n    gpu_name = torch.cuda.get_device_properties(0).name\n\n    return gpu_name\n\n\nactive_gpu_name = get_gpu_info()\n\nprint(\"GPU type:\")\nprint(active_gpu_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:25:33.892167Z","iopub.execute_input":"2025-07-27T03:25:33.892341Z","iopub.status.idle":"2025-07-27T03:25:35.737236Z","shell.execute_reply.started":"2025-07-27T03:25:33.892325Z","shell.execute_reply":"2025-07-27T03:25:35.736306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_id = \"PhanDai/luat-viet-nam-qa_small\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:25:35.738757Z","iopub.execute_input":"2025-07-27T03:25:35.739159Z","iopub.status.idle":"2025-07-27T03:25:35.742762Z","shell.execute_reply.started":"2025-07-27T03:25:35.739134Z","shell.execute_reply":"2025-07-27T03:25:35.741972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\ndtype = (\n    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n)\nif active_gpu_name and \"T4\" in active_gpu_name:\n    load_in_4bit = True  # Use 4bit quantization to reduce memory usage.\n    max_steps = 25  # Reduce training steps to avoiding waiting too long.\nelif active_gpu_name and (\"A100\" in active_gpu_name or \"L4\" in active_gpu_name):\n    load_in_4bit = False  # Disable 4bit quantization for faster training.\n    max_steps = 250  # As we train without 4bit quantization, we can train for more steps without waiting too long.\nelif active_gpu_name:\n    load_in_4bit = False  # Disable 4bit quantization for faster training.\n    max_steps = 150  # As we train without 4bit quantization, we can train for more steps without waiting too long.\nelse:\n    raise ValueError(\"No Nvidia GPU found.\")\n\nprint(\"--- Parameters ---\")\nprint(f\"{max_steps=}\")\nprint(f\"{load_in_4bit=}\")\nprint(f\"{dtype=}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:25:35.743568Z","iopub.execute_input":"2025-07-27T03:25:35.743767Z","iopub.status.idle":"2025-07-27T03:25:35.760790Z","shell.execute_reply.started":"2025-07-27T03:25:35.743750Z","shell.execute_reply":"2025-07-27T03:25:35.760058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nbase_model = \"1TuanPham/T-VisStar-7B-v0.1\"  # or unsloth/Qwen2.5-7B-Instruct\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=base_model,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:25:35.761576Z","iopub.execute_input":"2025-07-27T03:25:35.761785Z","iopub.status.idle":"2025-07-27T03:26:21.823666Z","shell.execute_reply.started":"2025-07-27T03:25:35.761751Z","shell.execute_reply":"2025-07-27T03:26:21.823038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    lora_alpha=16,\n    lora_dropout=0,  # Supports any, but = 0 is optimized\n    bias=\"none\",  # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  # We support rank stabilized LoRA\n    loftq_config=None,  # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:26:21.824372Z","iopub.execute_input":"2025-07-27T03:26:21.824567Z","iopub.status.idle":"2025-07-27T03:26:29.098940Z","shell.execute_reply.started":"2025-07-27T03:26:21.824552Z","shell.execute_reply":"2025-07-27T03:26:29.098140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\nalpaca_prompt = \"\"\"Dưới đây là hướng dẫn mô tả một nhiệm vụ, kết hợp với thông tin đầu vào cung cấp thêm ngữ cảnh. Hãy viết phản hồi hoàn thành yêu cầu một cách phù hợp.\n\n### Instruction:\nBạn là một trợ lý thông minh, hãy trả lời câu hỏi hiện tại của user dựa trên lịch sử chat và các tài liệu liên quan. Câu trả lời phải ngắn gọn, chính xác nhưng vẫn đảm bảo đầy đủ các ý chính.\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\n\ndef formatting_prompts_func(examples):\n    inputs = examples[\"question\"]\n    outputs = examples[\"context\"]\n    texts = []\n    for input_text, output in zip(inputs, outputs):\n        # Xử lý context nếu là list\n        if isinstance(output, list):\n            output_text = \" \".join(output)\n        else:\n            output_text = output\n\n        # Format prompt và thêm EOS token\n        formatted_text = alpaca_prompt.format(input_text.strip(), output_text.strip()) + EOS_TOKEN\n        texts.append(formatted_text)\n\n    return {\n        \"text\": texts,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:26:29.099796Z","iopub.execute_input":"2025-07-27T03:26:29.100044Z","iopub.status.idle":"2025-07-27T03:26:29.105565Z","shell.execute_reply.started":"2025-07-27T03:26:29.100026Z","shell.execute_reply":"2025-07-27T03:26:29.104788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(dataset_id)\ndataset = dataset.map(\n    formatting_prompts_func,\n    batched=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:26:29.106336Z","iopub.execute_input":"2025-07-27T03:26:29.106970Z","iopub.status.idle":"2025-07-27T03:28:12.027231Z","shell.execute_reply.started":"2025-07-27T03:26:29.106945Z","shell.execute_reply":"2025-07-27T03:28:12.026250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTTrainer\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset[\"train\"],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,  # Can make training 5x faster for short sequences.\n    args=TrainingArguments(\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        # num_train_epochs=1,  # Set this for 1 full training run, while commenting out 'max_steps'.\n        max_steps=max_steps,\n        # save_strategy=\"epoch\",\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"comet_ml\" if enable_comet else \"none\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:39:00.572640Z","iopub.execute_input":"2025-07-27T03:39:00.572949Z","iopub.status.idle":"2025-07-27T03:39:00.706918Z","shell.execute_reply.started":"2025-07-27T03:39:00.572919Z","shell.execute_reply":"2025-07-27T03:39:00.706397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:28:12.681135Z","iopub.execute_input":"2025-07-27T03:28:12.681308Z","iopub.status.idle":"2025-07-27T03:28:15.608586Z","shell.execute_reply.started":"2025-07-27T03:28:12.681294Z","shell.execute_reply":"2025-07-27T03:28:15.607908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:39:05.548060Z","iopub.execute_input":"2025-07-27T03:39:05.548627Z","iopub.status.idle":"2025-07-27T03:43:39.216827Z","shell.execute_reply.started":"2025-07-27T03:39:05.548603Z","shell.execute_reply":"2025-07-27T03:43:39.216139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\nmodel_name = f\"Chatbot_VietNamese_Law\"\nprint(f\"Model name: {model_name}\")\nmodel.save_pretrained_merged(\n    model_name,\n    tokenizer,\n    save_method=\"merged_16bit\",\n)  # Local saving\n\nif enable_hf:\n    api = HfApi()\n    user_info = api.whoami(token=hf_token)\n    huggingface_user = user_info[\"name\"]\n    print(f\"Current Hugging Face user: {huggingface_user}\")\n\n    model.push_to_hub_merged(\n        f\"{huggingface_user}/{model_name}\",\n        tokenizer=tokenizer,\n        save_method=\"merged_16bit\",\n        token=hf_token,\n    )  # Online saving to Hugging Face","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:43:39.218004Z","iopub.execute_input":"2025-07-27T03:43:39.218276Z","iopub.status.idle":"2025-07-27T03:53:36.781621Z","shell.execute_reply.started":"2025-07-27T03:43:39.218258Z","shell.execute_reply":"2025-07-27T03:53:36.781026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n\n# hf_token = \"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\"  # Thay bằng token của bạn\n# repo_id = \"AIPROENGINEER/Chatbot_VietNamese_Law\"  # VD: \"AIPROENGINEER/Chatbot_VietNamese_Law\"\n\n# api = HfApi()\n# api.delete_repo(\n#     repo_id = repo_id,\n#     token = hf_token,\n#     repo_type = \"model\",  # Rất quan trọng\n# )\n\n# print(f\"✅ Đã xoá model: {repo_id}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T09:17:25.115418Z","iopub.execute_input":"2025-07-27T09:17:25.115982Z","iopub.status.idle":"2025-07-27T09:17:26.128324Z","shell.execute_reply.started":"2025-07-27T09:17:25.115947Z","shell.execute_reply":"2025-07-27T09:17:26.127736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from transformers import TextStreamer\n\n# FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n# text_streamer = TextStreamer(tokenizer)\n\n\n# def generate_text(\n#     instruction, streaming: bool = True, trim_input_message: bool = False\n# ):\n#     message = alpaca_prompt.format(\n#         instruction,\n#         \"\",  # output - leave this blank for generation!\n#     )\n#     inputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n\n#     if streaming:\n#         return model.generate(\n#             **inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True\n#         )\n#     else:\n#         output_tokens = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n#         output = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n\n#         if trim_input_message:\n#             return output[len(message) :]\n#         else:\n#             return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:38:50.807620Z","iopub.status.idle":"2025-07-27T03:38:50.809143Z","shell.execute_reply.started":"2025-07-27T03:38:50.808952Z","shell.execute_reply":"2025-07-27T03:38:50.808972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate_text(dataset[\"validation\"][0][\"instruction\"], streaming=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:38:50.810086Z","iopub.status.idle":"2025-07-27T03:38:50.810708Z","shell.execute_reply.started":"2025-07-27T03:38:50.810555Z","shell.execute_reply":"2025-07-27T03:38:50.810573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n\n# model_name = f\"{base_model}-Chatbot_VietNamese_Law\"\n# print(f\"Model name: {model_name}\")\n# model.save_pretrained_merged(\n#     model_name,\n#     tokenizer,\n#     save_method=\"merged_16bit\",\n# )  # Local saving\n\n# if enable_hf:\n#     api = HfApi()\n#     user_info = api.whoami(token=hf_token)\n#     huggingface_user = user_info[\"name\"]\n#     print(f\"Current Hugging Face user: {huggingface_user}\")\n\n#     model.push_to_hub_merged(\n#         f\"{huggingface_user}/{model_name}\",\n#         tokenizer=tokenizer,\n#         save_method=\"merged_16bit\",\n#         token=hf_token,\n#     )  # Online saving to Hugging Face","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T03:38:50.811668Z","iopub.status.idle":"2025-07-27T03:38:50.811971Z","shell.execute_reply.started":"2025-07-27T03:38:50.811803Z","shell.execute_reply":"2025-07-27T03:38:50.811815Z"}},"outputs":[],"execution_count":null}]}