{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-27T09:16:45.302983Z",
     "iopub.status.busy": "2025-07-27T09:16:45.302832Z",
     "iopub.status.idle": "2025-07-27T09:17:01.605083Z",
     "shell.execute_reply": "2025-07-27T09:17:01.604357Z",
     "shell.execute_reply.started": "2025-07-27T09:16:45.302968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer comet_ml==3.48.1\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:25:33.885408Z",
     "iopub.status.busy": "2025-07-27T03:25:33.885162Z",
     "iopub.status.idle": "2025-07-27T03:25:33.891317Z",
     "shell.execute_reply": "2025-07-27T03:25:33.890578Z",
     "shell.execute_reply.started": "2025-07-27T03:25:33.885386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "hf_token = \"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\"\n",
    "enable_hf = bool(hf_token)\n",
    "print(f\"Is Hugging Face enabled? '{enable_hf}'\")\n",
    "\n",
    "comet_api_key = \"LHVLuczJDQUM8l4jzZVtRrpue\"\n",
    "enable_comet = bool(comet_api_key)\n",
    "comet_project_name = \"second-brain-course\"\n",
    "print(f\"Is Comet enabled? '{enable_comet}'\")\n",
    "\n",
    "if enable_hf:\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "if enable_comet:\n",
    "    os.environ[\"COMET_API_KEY\"] = comet_api_key\n",
    "    os.environ[\"COMET_PROJECT_NAME\"] = comet_project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:25:33.892341Z",
     "iopub.status.busy": "2025-07-27T03:25:33.892167Z",
     "iopub.status.idle": "2025-07-27T03:25:35.737236Z",
     "shell.execute_reply": "2025-07-27T03:25:35.736306Z",
     "shell.execute_reply.started": "2025-07-27T03:25:33.892325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_gpu_info() -> str | None:\n",
    "    \"\"\"Gets GPU device name if available.\n",
    "\n",
    "    Returns:\n",
    "        str | None: Name of the GPU device if available, None if no GPU is found.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "\n",
    "    gpu_name = torch.cuda.get_device_properties(0).name\n",
    "\n",
    "    return gpu_name\n",
    "\n",
    "\n",
    "active_gpu_name = get_gpu_info()\n",
    "\n",
    "print(\"GPU type:\")\n",
    "print(active_gpu_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:25:35.739159Z",
     "iopub.status.busy": "2025-07-27T03:25:35.738757Z",
     "iopub.status.idle": "2025-07-27T03:25:35.742762Z",
     "shell.execute_reply": "2025-07-27T03:25:35.741972Z",
     "shell.execute_reply.started": "2025-07-27T03:25:35.739134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_id = \"PhanDai/luat-viet-nam-qa_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:25:35.743767Z",
     "iopub.status.busy": "2025-07-27T03:25:35.743568Z",
     "iopub.status.idle": "2025-07-27T03:25:35.760790Z",
     "shell.execute_reply": "2025-07-27T03:25:35.760058Z",
     "shell.execute_reply.started": "2025-07-27T03:25:35.743750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "if active_gpu_name and \"T4\" in active_gpu_name:\n",
    "    load_in_4bit = True  # Use 4bit quantization to reduce memory usage.\n",
    "    max_steps = 25  # Reduce training steps to avoiding waiting too long.\n",
    "elif active_gpu_name and (\"A100\" in active_gpu_name or \"L4\" in active_gpu_name):\n",
    "    load_in_4bit = False  # Disable 4bit quantization for faster training.\n",
    "    max_steps = 250  # As we train without 4bit quantization, we can train for more steps without waiting too long.\n",
    "elif active_gpu_name:\n",
    "    load_in_4bit = False  # Disable 4bit quantization for faster training.\n",
    "    max_steps = 150  # As we train without 4bit quantization, we can train for more steps without waiting too long.\n",
    "else:\n",
    "    raise ValueError(\"No Nvidia GPU found.\")\n",
    "\n",
    "print(\"--- Parameters ---\")\n",
    "print(f\"{max_steps=}\")\n",
    "print(f\"{load_in_4bit=}\")\n",
    "print(f\"{dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:25:35.761785Z",
     "iopub.status.busy": "2025-07-27T03:25:35.761576Z",
     "iopub.status.idle": "2025-07-27T03:26:21.823666Z",
     "shell.execute_reply": "2025-07-27T03:26:21.823038Z",
     "shell.execute_reply.started": "2025-07-27T03:25:35.761751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "base_model = \"Qwen/Qwen2.5-7B-Instruct-1M\"  # or unsloth/Qwen2.5-7B-Instruct\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:26:21.824567Z",
     "iopub.status.busy": "2025-07-27T03:26:21.824372Z",
     "iopub.status.idle": "2025-07-27T03:26:29.098940Z",
     "shell.execute_reply": "2025-07-27T03:26:29.098140Z",
     "shell.execute_reply.started": "2025-07-27T03:26:21.824552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:26:29.100044Z",
     "iopub.status.busy": "2025-07-27T03:26:29.099796Z",
     "iopub.status.idle": "2025-07-27T03:26:29.105565Z",
     "shell.execute_reply": "2025-07-27T03:26:29.104788Z",
     "shell.execute_reply.started": "2025-07-27T03:26:29.100026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "alpaca_prompt = \"\"\"Dưới đây là hướng dẫn mô tả một nhiệm vụ, kết hợp với thông tin đầu vào cung cấp thêm ngữ cảnh. Hãy viết phản hồi hoàn thành yêu cầu một cách phù hợp.\n",
    "\n",
    "### Hướng dẫ:\n",
    "Bạn là một trợ lý thông minh, hãy trả lời câu hỏi hiện tại của user dựa trên lịch sử chat và các tài liệu liên quan. Câu trả lời phải ngắn gọn, chính xác nhưng vẫn đảm bảo đầy đủ các ý chính.\n",
    "### Câu hỏi:\n",
    "{}\n",
    "\n",
    "### Trả lời:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"question\"]\n",
    "    outputs = examples[\"context\"]\n",
    "    texts = []\n",
    "    for input_text, output in zip(inputs, outputs):\n",
    "        # Xử lý context nếu là list\n",
    "        if isinstance(output, list):\n",
    "            output_text = \" \".join(output)\n",
    "        else:\n",
    "            output_text = output\n",
    "\n",
    "        # Format prompt và thêm EOS token\n",
    "        formatted_text = alpaca_prompt.format(input_text.strip(), output_text.strip()) + EOS_TOKEN\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:26:29.106970Z",
     "iopub.status.busy": "2025-07-27T03:26:29.106336Z",
     "iopub.status.idle": "2025-07-27T03:28:12.027231Z",
     "shell.execute_reply": "2025-07-27T03:28:12.026250Z",
     "shell.execute_reply.started": "2025-07-27T03:26:29.106945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_id)\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:39:00.572949Z",
     "iopub.status.busy": "2025-07-27T03:39:00.572640Z",
     "iopub.status.idle": "2025-07-27T03:39:00.706918Z",
     "shell.execute_reply": "2025-07-27T03:39:00.706397Z",
     "shell.execute_reply.started": "2025-07-27T03:39:00.572919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs=1,  # Set this for 1 full training run, while commenting out 'max_steps'.\n",
    "        max_steps=max_steps,\n",
    "        # save_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"comet_ml\" if enable_comet else \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:28:12.681308Z",
     "iopub.status.busy": "2025-07-27T03:28:12.681135Z",
     "iopub.status.idle": "2025-07-27T03:28:15.608586Z",
     "shell.execute_reply": "2025-07-27T03:28:15.607908Z",
     "shell.execute_reply.started": "2025-07-27T03:28:12.681294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:39:05.548627Z",
     "iopub.status.busy": "2025-07-27T03:39:05.548060Z",
     "iopub.status.idle": "2025-07-27T03:43:39.216827Z",
     "shell.execute_reply": "2025-07-27T03:43:39.216139Z",
     "shell.execute_reply.started": "2025-07-27T03:39:05.548603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-27T03:43:39.218276Z",
     "iopub.status.busy": "2025-07-27T03:43:39.218004Z",
     "iopub.status.idle": "2025-07-27T03:53:36.781621Z",
     "shell.execute_reply": "2025-07-27T03:53:36.781026Z",
     "shell.execute_reply.started": "2025-07-27T03:43:39.218258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "model_name = f\"Chatbot_VietNamese_Law\"\n",
    "print(f\"Model name: {model_name}\")\n",
    "model.save_pretrained_merged(\n",
    "    model_name,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")  # Local saving\n",
    "\n",
    "if enable_hf:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami(token=hf_token)\n",
    "    huggingface_user = user_info[\"name\"]\n",
    "    print(f\"Current Hugging Face user: {huggingface_user}\")\n",
    "\n",
    "    model.push_to_hub_merged(\n",
    "        f\"{huggingface_user}/{model_name}\",\n",
    "        tokenizer=tokenizer,\n",
    "        save_method=\"merged_16bit\",\n",
    "        token=hf_token,\n",
    "    )  # Online saving to Hugging Face"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
